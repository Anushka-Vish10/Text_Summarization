{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Summarization_Spacy.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "iqbPud8MGjZ_"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from heapq import nlargest"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_gen(text_data):\n",
        "  stop_words = list(STOP_WORDS)\n",
        "  nlp = spacy.load('en_core_web_sm')\n",
        "  doc = nlp(text)\n",
        "  return doc"
      ],
      "metadata": {
        "id": "0rvKfruZHzNS"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def frequency_table(text_data):\n",
        "  \n",
        "  stop_words = list(STOP_WORDS)\n",
        "  doc = text_gen(text_data)\n",
        "  tokens = [token.text for token in doc]      # building list of the token\n",
        "  punct  = punctuation + '\\n'           #  adding /n also in punctuation list as it is not presnt in that pre-defined library\n",
        "  # creating word frequency table  i.e the count of each word\n",
        "  word_frequencies = {}\n",
        "  for word in doc:\n",
        "    if word.text.lower() not in stop_words:\n",
        "      if word.text.lower() not in punct:\n",
        "        if word.text not in word_frequencies.keys():  # even it should not be present in the keys of dictionary\n",
        "          word_frequencies[word.text] = 1\n",
        "        else:\n",
        "          word_frequencies[word.text] += 1\n",
        "  \n",
        "\n",
        "  # Taking the maximum frequency \n",
        "  max_frequency = max(word_frequencies.values())\n",
        "\n",
        "  # Creating frequency table\n",
        "  for word in word_frequencies.keys():\n",
        "    word_frequencies[word] = word_frequencies[word]/max_frequency\n",
        "  return word_frequencies\n"
      ],
      "metadata": {
        "id": "MTxKovmKIT9U"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_summarization(text_data):\n",
        "\n",
        "  doc = text_gen(text_data)\n",
        "\n",
        "  word_frequencies = frequency_table(text_data)\n",
        "\n",
        "  sentence_tokens = [sent for sent in doc.sents]\n",
        "  \n",
        "  sentence_scores = {}\n",
        "  for sent in sentence_tokens:\n",
        "    for word in sent:\n",
        "      if word.text.lower() in word_frequencies.keys():\n",
        "        # here we are going to add the word of maximum value sentences\n",
        "        if sent not in sentence_scores.keys():\n",
        "          sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
        "        else:\n",
        "          sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
        "  # Selecting the total length of \n",
        "  select_length = int(len(sentence_tokens) * 0.3)    # here we are selecting only 30% of the total sentences\n",
        "  # We need to select 6 sentences having  maximum frequency count\n",
        "  summary = nlargest(select_length, sentence_scores, key = sentence_scores.get)\n",
        "\n",
        "  # Now we need to combine these sentences\n",
        "  final_summary = [word.text for word in summary]\n",
        "\n",
        "\n",
        "  summarization = \" \".join(final_summary)\n",
        "  return  summarization\n"
      ],
      "metadata": {
        "id": "mX_dkliIMjRa"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_data = ''' \n",
        "Anushka Sharma confesses she's not having the easiest time learning cricketAnushka Sharma has shared a new selfie from the prep for her upcoming cricket-based film Chakda Express. The actor is evidently having a tough time getting the hang of cricket moves.Anushka Sharma is not having the easiest time polishing. her cricket skills before she begins shoot for Chakda 'Xpress. On Wednesday, she shared a selfie as she took a short rest after a practice session. The selfie showed her in a white T-shirt with her hair tied in the back. She had her fist against her lips and looked in the camera. (Also read: Anushka Sharma reveals she turns to husband Virat Kohli for batting tips as she preps for Chakda Xpress)“Kaash bachpan mein kuch toh cricket khela hota toh aaj haalat aise na hoti (I wish I had played some cricket in my childhood, perhaps then I would not have suffered like this today),” she wrote with her photo. Anushka plays cricket star Jhulan Goswami in the movie.Chakda 'Xpress was announced in January with a teaser. Talking about the film, Anushka wrote in her post: “Chakda Xpress is inspired by the life and times of former Indian captain Jhulan Goswami and it will be an eye-opener into the world of women’s cricket. At a time when Jhulan decided to become a cricketer and make her country proud on the global stage, it was very tough for women to even think of playing the sport. This film is a dramatic retelling of several instances that shaped her life and also women’s cricket.” Directed by Prosit Roy, Chakda Xpress is based on the life of Jhulan, who became the second Indian woman cricketer to receive the Padma Shri, in 2012.The film marks Anushka's comeback to acting after four years. She was last seen with Shah Rukh Khan in Zero, which released in 2018. She took a long break after that and even welcomed her first child, daughter Vamika.Anushka recently told Harper’s Bazaar in an interview that she and cricketer-husband Virat Kohli discuss her progress for her new film, as she shows him her cricket videos. She said, \"We definitely discuss my progress. Whenever I’ve had a good day learning, I like to share my videos with Virat, to get his feedback. Luckily, he’s not a bowler so I listen to my coach more. But I do turn to Virat for batting tips. \"\n",
        "'''"
      ],
      "metadata": {
        "id": "Ez4MLDXFLlzC"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_summarization(text_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "D9FxnI6lJSWu",
        "outputId": "df8dcc41-9671-46b8-da5a-8d240af5ec38"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In this course you’ll learn how to use spaCy to build advanced natural language understanding systems, using both rule-based and machine learning approaches. While some of spaCy’s features work independently, others require trained pipelines to be loaded, which enable spaCy to predict linguistic annotations – for example, whether a word is a verb or a noun. A trained pipeline can consist of multiple components that use a statistical model trained on labeled data. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning.\\n spaCy is designed specifically for production use and helps you build applications that process and For a general-purpose use case, the small, default packages are always a good start\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this course you’ll learn how to use spaCy to build advanced natural language understanding systems, using both rule-based and machine learning approaches. While some of spaCy’s features work independently, others require trained pipelines to be loaded, which enable spaCy to predict linguistic annotations – for example, whether a word is a verb or a noun. A trained pipeline can consist of multiple components that use a statistical model trained on labeled data. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning.\n",
        " spaCy is designed specifically for production use and helps you build applications that process and For a general-purpose use case, the small, default packages are always a good start\n"
      ],
      "metadata": {
        "id": "y6OvibD8U59x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For cross checking"
      ],
      "metadata": {
        "id": "YXHcE3mmPSYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(text_data))\n",
        "print(len(text_summarization(text_data)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# As we can have a clear vision i.e. summarized text is almost around 30% of actuall text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eI5XN4dPUww",
        "outputId": "8238d2de-d938-49a3-d254-c294ded6efb8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1909\n",
            "785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "y_XJPi8MPdXQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}