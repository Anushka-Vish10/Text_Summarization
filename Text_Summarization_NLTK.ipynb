{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Summarization_NLTK.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBYyjWkSP4Kp",
        "outputId": "1ba3ad3a-e813-421f-bb0b-407bca1c275e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import networkx as nx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEOcqkRoRrUv",
        "outputId": "b4eb858b-3c3c-471b-f7dd-2578bedc0c2f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for reading an article/text/Context\n",
        "\n",
        "\n",
        "def read_article(file_name):\n",
        "  \n",
        "  file = open(file_name, \"r\")\n",
        "  filedata = file.readlines()\n",
        "  article = filedata[0].split(\". \")  # here dot act as a delimiter\n",
        "\n",
        "  sentences = []   # we will be storing the individual sentences\n",
        "  for sentence in article:\n",
        "    sentences.append(sentence.replace(\"[^a-zA-Z]\",\" \").split(\" \"))\n",
        "\n",
        "  sentences.pop()\n",
        "  return sentences"
      ],
      "metadata": {
        "id": "RQsKgpqSSE2v"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_similarity(sent1, sent2, stopwords = None):\n",
        "  if stopwords is None:\n",
        "    stopwords = []\n",
        "\n",
        "  # Creating corpus for sentence1 and 2\n",
        "  sent1 = [w.lower() for w in sent1]\n",
        "  sent2 = [w.lower() for w in sent2]\n",
        "\n",
        "  # For getting only unique words from sentence1 and sentence2\n",
        "  all_words = (list(set(sent1+sent2)))\n",
        "\n",
        "  # Creating two vectors as well\n",
        "  vector1 = [0] * len(all_words)\n",
        "  vector2 = [0] * len(all_words)\n",
        "\n",
        "  for w in sent1:\n",
        "    if w in stopwords:\n",
        "      continue\n",
        "    # Enumurating vector \n",
        "    vector1[all_words.index(w)]+=1\n",
        "  \n",
        "  for w in sent2:\n",
        "    if w in stopwords:\n",
        "      continue\n",
        "    vector2[all_words.index(w)]+=1\n",
        "\n",
        "    # for getting sentence similarity\n",
        "    return 1-cosine_distance(vector1,vector2)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bw1BLbMfSI3f"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generating similarity matrix\n",
        "def gen_similar_matrix(sentences, stop_words):\n",
        "\n",
        "  # creating empty similarity matrix\n",
        "  # since it is a 2d matrix , rows having len of sentences, column having len of sentences\n",
        "\n",
        "  similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "  # creating matrix\n",
        "  for idx1 in range(len(sentences)):\n",
        "    for idx2 in range(len(sentences)):\n",
        "      if idx1 == idx2:\n",
        "        continue\n",
        "      similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "  return similarity_matrix\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Sl8xy9ulSMWN"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# taking first 5 lines\n",
        "\n",
        "def generate_summary(file_name, top_n = 5): \n",
        "  stop_words = stopwords.words('english')\n",
        "  summarize_text = []\n",
        "  sentences = read_article(file_name)\n",
        "  # generate similarity matrix\n",
        "  sentence_similarity_matrix = gen_similar_matrix(sentences, stop_words)\n",
        "  sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
        "\n",
        "  scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "  # ranking the sentences and pick-up the top sentences\n",
        "  ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse = True)\n",
        "  \n",
        "  # Need summary to be precise, so iterating through top_5 sent\n",
        "  for i in range(top_n):\n",
        "    summarize_text.append(\" \".join(ranked_sentences[i][1]))\n",
        "  print(\"Summary \\n\",\". \".join(summarize_text))\n",
        "\n"
      ],
      "metadata": {
        "id": "kGl1IOLKSO7x"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = \"gen_news.txt\"\n",
        "top_n = 2\n",
        "generate_summary(file_name, top_n = 5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSgE0PPKSRXj",
        "outputId": "b65afbff-f9ed-4c5e-be1a-fbcb54bf77e9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary \n",
            " Anushka Sharma confesses she's not having the easiest time learning cricketAnushka Sharma has shared a new selfie from the prep for her upcoming cricket-based film Chakda Express. her cricket skills before she begins shoot for Chakda 'Xpress. Anushka plays cricket star Jhulan Goswami in the movie.Chakda 'Xpress was announced in January with a teaser. On Wednesday, she shared a selfie as she took a short rest after a practice session. The actor is evidently having a tough time getting the hang of cricket moves.Anushka Sharma is not having the easiest time polishing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anushka Sharma confesses she's not having the easiest time learning cricketAnushka Sharma has shared a new selfie from the prep for her upcoming cricket-based film Chakda Express. her cricket skills before she begins shoot for Chakda 'Xpress. Anushka plays cricket star Jhulan Goswami in the movie.Chakda 'Xpress was announced in January with a teaser. On Wednesday, she shared a selfie as she took a short rest after a practice session. The actor is evidently having a tough time getting the hang of cricket moves.Anushka Sharma is not having the easiest time polishing"
      ],
      "metadata": {
        "id": "ZssbVdf5V7ol"
      }
    }
  ]
}